---
title: "Written Assignment 08"
author: "Simone Mayers"
date: today
date-format: long
number-sections: true
number-depth: 3
fig-cap-location: margin
format:
  html:
    toc: true
    toc-location: right
    number-sections: true
    number-depth: 3
    html-math-method: katex
    embed-resources: true
    self-contained: true
# bibliography: dasc-6000.bib 
# csl: ieee-with-url.csl
# linkcolor: red
# urlcolor: blue
# link-citations: yes
# header-includes:
#   - \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

# Assignment Goal {.unnumbered}

The overarching goal for this assignment is to assess your understanding of evaluating machine learning models. 

# Instructions {.unnumbered}

Please show all your work. Simply providing the final answer is treated as no response. If you do not use R or Python notebooks, it is fine. However, the document structure should be preserved if you choose to use Microsoft Word or something else.

Please submit your response either as a self-contained HTML or PDF document.

There are two questions. Each question carries 50 points.

# Basic Evaluation Measures - Categorical Target

The table below shows the predictions made for a **categorical target feature** by a model for a test dataset.

```{r}
#| echo: false
library(readr)
df1 <- readr::read_csv("./categorical-target.csv", col_names = TRUE)
```


```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df1, caption = 'Predictions made for a categorical target feature.', align = "lrr")
```


Based on this test set, calculate the following evaluation measures.

(a) A confusion matrix and the misclassification rate

    ```{r}
    # Calculate confusion matrix components
    TP <- sum(df1$Target == "TRUE" & df1$Prediction == "TRUE")
    TN <- sum(df1$Target == "FALSE" & df1$Prediction == "FALSE")
    FP <- sum(df1$Target == "FALSE" & df1$Prediction == "TRUE")
    FN <- sum(df1$Target == "TRUE" & df1$Prediction == "FALSE")
    
    # Create the confusion matrix
    confusion_matrix <- matrix(c(TN, FP, FN, TP), 
                               nrow = 2, 
                               dimnames = list("Actual" = c("FALSE", "TRUE"),
                                               "Predicted" = c("FALSE", "TRUE")))
    
    # Calculate misclassification rate
    misclassification_rate <- (FP + FN) / nrow(df1)
    
    # Display results
    knitr::kable(confusion_matrix, caption = 'Confusion Matrix for Predictions', align = "l")
    cat(paste("Misclassification Rate:", round(misclassification_rate, 3)))

    ```
    
(b) The average class accuracy (harmonic mean)

    ```{r}
    # Calculate Precision and Recall
    precision <- TP / (TP + FP)
    recall <- TP / (TP + FN)
    
    # Calculate Harmonic Mean (F1-score)
    f1_score <- 2 * (precision * recall) / (precision + recall)
    
    # Display the results
    cat(paste("Precision:", round(precision, 3)))
    cat(paste("\nRecall:", round(recall, 3)))
    cat(paste("\nHarmonic Mean (F1-score):", round(f1_score, 3)))
    ```

(c) The precision, recall, and $F_1$ measure

    ```{r}
    # Calculate Precision
    precision <- TP / (TP + FP)
    
    # Calculate Recall
    recall <- TP / (TP + FN)
    
    # Calculate F1 Measure
    f1_measure <- 2 * (precision * recall) / (precision + recall)
    
    # Display results
    cat(paste("Precision:", round(precision, 3)))
    cat(paste("\nRecall:", round(recall, 3)))
    cat(paste("\nF1 Measure:", round(f1_measure, 3)))
    ```



# Basic Evaluation Measures - Continuous Target

The table below shows the predictions made for a **continuous target feature** by two different prediction models for a test dataset.

```{r}
#| echo: false
library(readr)
df2 <- readr::read_csv("./continuous-target.csv", col_names = TRUE)
```


```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df2, caption = 'Predictions made for a continuous target feature.', align = "lrrr")
```

(a) Based on these predictions, calculate the evaluation measures listed below for each
model.

    i. The sum of squared errors
    
    ```{r}
    # Calculate Sum of Squared Errors (SSE) for Model 1
    df2$SSE_Model1 <- (df2$Target - df2$`Model 1 Prediction`)^2
    SSE_Model1 <- sum(df2$SSE_Model1)
    
    # Calculate Sum of Squared Errors (SSE) for Model 2
    df2$SSE_Model2 <- (df2$Target - df2$`Model 2 Prediction`)^2
    SSE_Model2 <- sum(df2$SSE_Model2)
    
    # Display results
    cat(paste("Sum of Squared Errors for Model 1:", SSE_Model1))
    cat(paste("\nSum of Squared Errors for Model 2:", SSE_Model2))
    ```
    
    ii. The $R^2$ measure

    ```{r}
    # Calculate the mean of the Target
    target_mean <- mean(df2$Target)
    
    # Calculate Total Sum of Squares (SST)
    df2$SST <- (df2$Target - target_mean)^2
    SST <- sum(df2$SST)
    
    # Calculate SSE for each model (if not already computed)
    df2$SSE_Model1 <- (df2$Target - df2$`Model 1 Prediction`)^2
    df2$SSE_Model2 <- (df2$Target - df2$`Model 2 Prediction`)^2
    SSE_Model1 <- sum(df2$SSE_Model1)
    SSE_Model2 <- sum(df2$SSE_Model2)
    
    # Calculate R^2 for each model
    R2_Model1 <- 1 - (SSE_Model1 / SST)
    R2_Model2 <- 1 - (SSE_Model2 / SST)
    
    # Display results
    cat(paste("R^2 for Model 1:", round(R2_Model1, 3)))
    cat(paste("\nR^2 for Model 2:", round(R2_Model2, 3)))
    ```

(b) Based on the evaluation measures calculated, which model do you think is performing better for this dataset?


    Model 1 has a significantly lower SSE (33,500 vs. 94,738) and a higher 𝑅2 (0.928 vs. 0.798) compared to Model 2. Therefore, Model 1 is the better-performing model for this dataset, as it is more accurate and explains more variance in the target variable.



